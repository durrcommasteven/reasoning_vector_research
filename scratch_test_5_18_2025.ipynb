{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# -----------------------------------------\n",
    "# Configuration of comparisons\n",
    "# -----------------------------------------\n",
    "COMPARISONS = [\n",
    "    {\"name\": \"base_answer_vs_enhanced_reason_body\",       \"mode1\": \"base\",              \"region1\": \"answering\", \"mode2\": \"reasoning_boosted\",\"region2\": \"reasoning\", \"subset\": \"body\",    \"normalize\": True},\n",
    "    {\"name\": \"base_reason_vs_base_answer_body\",           \"mode1\": \"base\",              \"region1\": \"reasoning\", \"mode2\": \"base\",              \"region2\": \"answering\",  \"subset\": \"body\",    \"normalize\": True},\n",
    "    {\"name\": \"enhanced_reason_vs_enhanced_answer_body\",   \"mode1\": \"reasoning_boosted\",\"region1\": \"reasoning\", \"mode2\": \"reasoning_boosted\",\"region2\": \"answering\", \"subset\": \"body\",    \"normalize\": True},\n",
    "    {\"name\": \"base_reason_vs_immediate_answer_body\",      \"mode1\": \"base\",              \"region1\": \"reasoning\", \"mode2\": \"immediate_answer\",\"region2\": \"answering\", \"subset\": \"body\",    \"normalize\": True},\n",
    "    {\"name\": \"enhanced_reason_vs_base_reason_body\",       \"mode1\": \"reasoning_boosted\",\"region1\": \"reasoning\", \"mode2\": \"base\",              \"region2\": \"reasoning\",  \"subset\": \"body\",    \"normalize\": True},\n",
    "    {\"name\": \"base_reason_vs_base_answer_initial\",        \"mode1\": \"base\",              \"region1\": \"reasoning\", \"mode2\": \"base\",              \"region2\": \"answering\",  \"subset\": \"initial\", \"normalize\": False},\n",
    "    {\"name\": \"enhanced_reason_vs_enhanced_answer_initial\",\"mode1\": \"reasoning_boosted\",\"region1\": \"reasoning\", \"mode2\": \"reasoning_boosted\",\"region2\": \"answering\", \"subset\": \"initial\", \"normalize\": False},\n",
    "    {\"name\": \"enhanced_reason_vs_base_reason_initial\",    \"mode1\": \"reasoning_boosted\",\"region1\": \"reasoning\", \"mode2\": \"base\",              \"region2\": \"reasoning\",  \"subset\": \"initial\", \"normalize\": False},\n",
    "    {\"name\": \"base_reason_vs_immediate_answer_initial\",   \"mode1\": \"base\",              \"region1\": \"reasoning\", \"mode2\": \"immediate_answer\",\"region2\": \"answering\", \"subset\": \"initial\", \"normalize\": False},\n",
    "]\n",
    "\n",
    "# -----------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------\n",
    "\n",
    "def load_tensor(prompt_folder: str, mode: str) -> np.ndarray:\n",
    "    \"\"\"Load and convert bf16 tensor to float32 numpy.\"\"\"\n",
    "    pt_path = os.path.join(prompt_folder, f\"{mode}.pt\")\n",
    "    return torch.load(pt_path).float().cpu().numpy()\n",
    "\n",
    "\n",
    "def load_metadata(prompt_folder: str, mode: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load YAML metadata only.\"\"\"\n",
    "    yaml_path = os.path.join(prompt_folder, f\"{mode}_metadata.yaml\")\n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def filter_indices(metadata: List[Dict[str, Any]], region: str, subset: str) -> List[int]:\n",
    "    \"\"\"Return indices matching region/subset.\"\"\"\n",
    "    idxs = []\n",
    "    for i, m in enumerate(metadata):\n",
    "        if m['region'] != region:\n",
    "            continue\n",
    "        dist = m.get('dist_from_prev_marker')\n",
    "        if subset == 'body' and dist is not None and dist > 1:\n",
    "            idxs.append(i)\n",
    "        elif subset == 'initial' and dist == 1:\n",
    "            idxs.append(i)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def compute_mean_diff(resid: np.ndarray, idx1: List[int], idx2: List[int]) -> np.ndarray:\n",
    "    \"\"\"Mean difference vector (mean1 - mean2) per layer.\"\"\"\n",
    "    return resid[idx1].mean(axis=0) - resid[idx2].mean(axis=0)\n",
    "\n",
    "\n",
    "def compute_lda_dir(resid: np.ndarray, idx1: List[int], idx2: List[int]) -> np.ndarray:\n",
    "    \"\"\"LDA discriminant direction per layer.\"\"\"\n",
    "    n_layers = resid.shape[1]\n",
    "    dirs = []\n",
    "    for l in range(n_layers):\n",
    "        X = np.concatenate([resid[idx1, l, :], resid[idx2, l, :]], axis=0)\n",
    "        y = np.hstack([np.zeros(len(idx1)), np.ones(len(idx2))])\n",
    "        lda = LinearDiscriminantAnalysis(solver='eigen')\n",
    "        lda.fit(X, y)\n",
    "        dirs.append(lda.coef_[0])\n",
    "    return np.stack(dirs, axis=0)\n",
    "\n",
    "\n",
    "def token_normalize(resid: np.ndarray,\n",
    "                    metadata: List[Dict[str, Any]],\n",
    "                    idx1: List[int],\n",
    "                    idx2: List[int],\n",
    "                    valid_ids: Set[int]) -> np.ndarray:\n",
    "    \"\"\"Subtract token-specific bias based on token_id.\"\"\"\n",
    "    resid_norm = resid.copy()\n",
    "    positions: Dict[int, List[int]] = {}\n",
    "    for idx in idx1 + idx2:\n",
    "        tok_id = metadata[idx]['token_id']\n",
    "        if tok_id in valid_ids:\n",
    "            positions.setdefault(tok_id, []).append(idx)\n",
    "    for tok_id, pos_list in positions.items():\n",
    "        bias = resid[pos_list, :, :].mean(axis=0)\n",
    "        for pos in pos_list:\n",
    "            resid_norm[pos] -= bias\n",
    "    return resid_norm\n",
    "\n",
    "\n",
    "def compute_common_tokens(data_root: str,\n",
    "                          mode1: str, region1: str,\n",
    "                          mode2: str, region2: str) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Find common token_ids between two (mode, region) pairs across all prompts.\n",
    "    \"\"\"\n",
    "    prompt_folders = sorted(\n",
    "        os.path.join(data_root, d)\n",
    "        for d in os.listdir(data_root)\n",
    "        if os.path.isdir(os.path.join(data_root, d))\n",
    "    )\n",
    "    set1, set2 = set(), set()\n",
    "    for folder in prompt_folders:\n",
    "        meta1 = load_metadata(folder, mode1)\n",
    "        for m in meta1:\n",
    "            if m['region'] == region1:\n",
    "                set1.add(m['token_id'])\n",
    "        meta2 = load_metadata(folder, mode2)\n",
    "        for m in meta2:\n",
    "            if m['region'] == region2:\n",
    "                set2.add(m['token_id'])\n",
    "    return set1 & set2\n",
    "\n",
    "# -----------------------------------------\n",
    "# Analysis Pipeline\n",
    "# -----------------------------------------\n",
    "\n",
    "def run_analysis(data_root: str, out_root: str, per_subject: bool = False):\n",
    "    \"\"\"\n",
    "    Perform overall or per-subject analyses.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    prompt_folders = sorted(\n",
    "        os.path.join(data_root, d)\n",
    "        for d in os.listdir(data_root)\n",
    "        if os.path.isdir(os.path.join(data_root, d))\n",
    "    )\n",
    "    subject_map: Dict[str, List[str]] = {}\n",
    "    for folder in prompt_folders:\n",
    "        subj = os.path.basename(folder).split('_', 1)[0]\n",
    "        subject_map.setdefault(subj, []).append(folder)\n",
    "    scopes = {'ALL': prompt_folders}\n",
    "    if per_subject:\n",
    "        for subj, fl in subject_map.items(): scopes[subj] = fl\n",
    "\n",
    "    for scope, folders in scopes.items():\n",
    "        for comp in COMPARISONS:\n",
    "            name = comp['name']\n",
    "            m1, r1 = comp['mode1'], comp['region1']\n",
    "            m2, r2 = comp['mode2'], comp['region2']\n",
    "            subset, normalize = comp['subset'], comp['normalize']\n",
    "\n",
    "            resid_list1, resid_list2 = [], []\n",
    "            meta_list1, meta_list2 = [], []\n",
    "            cum1 = 0  # offset for second group\n",
    "            for fld in folders:\n",
    "                meta1 = load_metadata(fld, m1)\n",
    "                meta2 = load_metadata(fld, m2)\n",
    "                idx1 = filter_indices(meta1, r1, subset)\n",
    "                idx2 = filter_indices(meta2, r2, subset)\n",
    "                if not idx1 or not idx2: continue\n",
    "                r1_arr = load_tensor(fld, m1)\n",
    "                r2_arr = load_tensor(fld, m2)\n",
    "                resid_list1.append(r1_arr)\n",
    "                resid_list2.append(r2_arr)\n",
    "                meta_list1.extend(meta1)\n",
    "                meta_list2.extend(meta2)\n",
    "                cum1 += r1_arr.shape[0]\n",
    "\n",
    "            if not resid_list1: continue\n",
    "            combined1 = np.concatenate(resid_list1, axis=0)\n",
    "            combined2 = np.concatenate(resid_list2, axis=0)\n",
    "            combined_meta = meta_list1 + meta_list2\n",
    "\n",
    "            idx1_comb = []\n",
    "            offset = 0\n",
    "            for arr, fld in zip(resid_list1, folders):\n",
    "                ids = filter_indices(load_metadata(fld, m1), r1, subset)\n",
    "                idx1_comb += [offset + i for i in ids]\n",
    "                offset += arr.shape[0]\n",
    "            idx2_comb = []\n",
    "            for arr, fld in zip(resid_list2, folders):\n",
    "                ids = filter_indices(load_metadata(fld, m2), r2, subset)\n",
    "                idx2_comb += [offset + i for i in ids]\n",
    "                offset += arr.shape[0]\n",
    "\n",
    "            if normalize:\n",
    "                valid_ids = compute_common_tokens(data_root, m1, r1, m2, r2)\n",
    "                big = np.concatenate([combined1, combined2], axis=0)\n",
    "                norm = token_normalize(big, combined_meta, idx1_comb,\n",
    "                                       [i+combined1.shape[0] for i in idx2_comb],\n",
    "                                       valid_ids)\n",
    "                c1 = norm[:combined1.shape[0]]\n",
    "                c2 = norm[combined1.shape[0]:]\n",
    "            else:\n",
    "                c1, c2 = combined1, combined2\n",
    "\n",
    "            mean_diff = compute_mean_diff(c1, idx1_comb, idx2_comb)\n",
    "            lda_dir   = compute_lda_dir(c1, idx1_comb, idx2_comb)\n",
    "\n",
    "            outd = os.path.join(out_root, scope, name)\n",
    "            os.makedirs(outd, exist_ok=True)\n",
    "            np.save(os.path.join(outd, 'mean_diff.npy'), mean_diff)\n",
    "            np.save(os.path.join(outd, 'lda_dir.npy'), lda_dir)\n",
    "\n",
    "    print(\"Analysis complete. Results saved in\", out_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_tokens = compute_common_tokens(\"reasoning_resid_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1368, 0, 1070]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(v) for k, v in common_tokens.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
